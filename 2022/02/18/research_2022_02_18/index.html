<!doctype html>
<html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM - 金子の進捗</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="金子の進捗"><meta name="msapplication-TileImage" content="/img/self_face.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="金子の進捗"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="💡 概要  訓練可能な層を訓練不要な層に置換することによる高速化 評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析 学習設定がデータスケーリングの法則に与える影響 大規模多言語言語モデルXGLMのfew-shot学習とzero-shot学習の調査"><meta property="og:type" content="blog"><meta property="og:title" content="論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM"><meta property="og:url" content="https://masahiro-kaneko.com/2022/02/18/research_2022_02_18/"><meta property="og:site_name" content="金子の進捗"><meta property="og:description" content="💡 概要  訓練可能な層を訓練不要な層に置換することによる高速化 評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析 学習設定がデータスケーリングの法則に与える影響 大規模多言語言語モデルXGLMのfew-shot学習とzero-shot学習の調査"><meta property="og:locale" content="ja_JP"><meta property="og:image" content="https://masahiro-kaneko.com/images/survey.png"><meta property="article:published_time" content="2022-02-17T15:00:00.000Z"><meta property="article:modified_time" content="2022-02-17T20:47:23.953Z"><meta property="article:author" content="金子 正弘"><meta property="article:tag" content="論文紹介"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/images/survey.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://masahiro-kaneko.com/2022/02/18/research_2022_02_18/"},"headline":"論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM","image":["https://masahiro-kaneko.com/images/survey.png"],"datePublished":"2022-02-17T15:00:00.000Z","dateModified":"2022-02-17T20:47:23.953Z","author":{"@type":"Person","name":"金子 正弘"},"description":"💡 概要  訓練可能な層を訓練不要な層に置換することによる高速化 評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析 学習設定がデータスケーリングの法則に与える影響 大規模多言語言語モデルXGLMのfew-shot学習とzero-shot学習の調査"}</script><link rel="canonical" href="https://masahiro-kaneko.com/2022/02/18/research_2022_02_18/"><link rel="icon" href="/img/self_face.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-3DTPLMK6B0" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-3DTPLMK6B0');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-4881949383702908" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/title.png" alt="金子の進捗" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="検索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/images/survey.png" alt="論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-02-17T15:00:00.000Z" title="2/18/2022, 12:00:00 AM">2022-02-18</time>に投稿</span><span class="level-item"><time dateTime="2022-02-17T20:47:23.953Z" title="2/18/2022, 5:47:23 AM">2022-02-18</time>に更新</span><span class="level-item"><a class="link-muted" href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></span><span class="level-item">14分で読む (約2123語)</span></div></div><h1 class="title is-3 is-size-4-mobile">論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM</h1><div class="content"><h1><span id="概要">💡 <strong>概要</strong></span></h1>
<ul>
<li>訓練可能な層を訓練不要な層に置換することによる高速化</li>
<li>評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析</li>
<li>学習設定がデータスケーリングの法則に与える影響</li>
<li>大規模多言語言語モデルXGLMのfew-shot学習とzero-shot学習の調査</li>
</ul>
<span id="more"></span>
<h1><span id="訓練可能な層を訓練不要な層に置換することによる高速化"><strong>訓練可能な層を訓練不要な層に置換することによる高速化</strong></span></h1>
<blockquote>
<p>タイトル： Learning Features with Parameter-Free Layers<br>
著者：Dongyoon Han, YoungJoon Yoo, Beomyoung Kim, Byeongho Heo<br>
会議・出版： ICLR<br>
年： 2022<br>
<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=bCrdi4iVvv"><i class="far fa-file-pdf"></i></a></p>
</blockquote>
<p>畳み込みブロックのような学習可能な層は、連続した空間の演算によってグローバルな文脈を捉えるためのパラメータを学習しており、標準的なネットワーク設計の選択肢となっている。効率的なネットワークを設計する場合、深さ方向の畳み込みなどの学習可能な層はパラメータ数とFLOPsの効率化が可能であるが、実際にはモデル速度の向上はほとんど得られない。</p>
<p>本論文では、ネットワーク構造において演算を効率的な訓練可能な層に置き換えるのではなく、シンプルな訓練不要なパラメータフリー演算に置き換えることが速度に関して有効であることを明らかにする。これはネットワーク構造の演算を訓練可能な層で構成するという固定観念を打破することを目的とする。</p>
<p>max-poolやavg-poolのような訓練不要な演算が機能するかどうかを調べるために、訓練されたモデルによる層レベルの調査とネットワーク構造の探索により訓練不要な演算の置換について広範な実験分析を行う。この調査により、モデルの精度をそれほど犠牲にすることなく、訓練不要な演算を主要な構成要素としてモデルに用いるアイデアを得る。ImageNetデータセットにおける実験の結果、訓練不要な演算を用いたネットワーク構造は、モデル速度、パラメータ数、FLOPsの面でさらなる効率化のメリットを享受できることが実証された。</p>
<h1><span id="評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析"><strong>評価モデルに対して翻訳モデルを最適化することで評価モデルの弱点を分析</strong></span></h1>
<blockquote>
<p>タイトル： Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET<br>
著者：Chantal Amrhein, Rico Sennrich<br>
会議・出版： arXiv<br>
年： 2022<br>
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.05148"><i class="far fa-file-pdf"></i></a></p>
</blockquote>
<p><img src="/images/weakness_of_COMET.png" alt="MBRデコーディングを用いた出力の数字や名前に関する問題点"></p>
<p>機械翻訳においてニューラルネットワークを用いた評価指標は人間の評価と高い相関を実現している。一方で、このような評価指標に対して機械翻訳モデルを最適化する前に、高いスコアを得た悪い翻訳へのバイアスを認識し排除する必要がある。</p>
<p>この論文ではサンプルベースのMinimum Bayes Risk (MBR) デコーディングを使用し、評価指標の弱点を見つけ定量化することができることを示した。実験では英語-&gt;ドイツ語とドイツ語-&gt;英語の翻訳に対して、ニューラルネットワークを用いた評価指標であるCOMETをMBRデコーディングに適用した。その結果、COMETは数字や名前の不一致に十分な評価を行えないことが明らかになった。さらに、これらの問題は単に擬似データを追加学習させるだけでは完全に除去できないことも示した。</p>
<h1><span id="学習設定がデータスケーリングの法則に与える影響"><strong>学習設定がデータスケーリングの法則に与える影響</strong></span></h1>
<blockquote>
<p>タイトル： Data Scaling Laws in NMT: The Effect of Noise and Architecture<br>
著者：Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin Cherry, Behnam Neyshabur, Orhan Firat<br>
会議・出版： arXiv<br>
年： 2022<br>
<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=AB2r0YKBSpD"><i class="far fa-file-pdf"></i></a></p>
</blockquote>
<p><img src="/images/scale_law.png" alt="データサイズとテスト性能"></p>
<p>本研究では、ニューラル機械翻訳のデータスケーリングの法則を実証的に示す。まず、エンコーダ・デコーダTransformerモデルのテストデータにおける損失が、モデルサイズに依存し学習サンプル数のべき乗則でスケールすることを明らかにする。次に、学習設定の様々な要素を変更しデータスケーリングの法則にどのような影響を与えるかを調査した。</p>
<p>その結果、変更の大部分はスケーリング曲線の乗法的なシフトをもたらすだけで、指数はほとんど変化しないことがわかった。これはモデル構造や学習データの品質が多少悪くとも、データを追加することで補正できることを示唆している。一方で、パラレルデータではなく逆翻訳データを用いて学習分布を変更すると、スケーリング指数に影響を与えることがわかった。</p>
<h1><span id="大規模多言語言語モデルxglmのfew-shot学習とzero-shot学習の調査"><strong>大規模多言語言語モデルXGLMのfew-shot学習とzero-shot学習の調査</strong></span></h1>
<blockquote>
<p>タイトル： Few-shot Learning with Multilingual Language Models<br>
著者：Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li<br>
会議・出版： arXiv<br>
年： 2022<br>
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10668"><i class="far fa-file-pdf"></i></a> <a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq/tree/main/examples/xglm"><i class="fab fa-github"></i></a></p>
</blockquote>
<p>GPT-3などの大規模言語モデルは、fine-tuningなしに広範囲のタスクを処理することができる。大規模言語モデルは多くの異なる言語を同一空間上で表現できることが知られているが、学習データは英語が支配的であり言語横断的な汎化性には限界がある可能性がある。</p>
<p>本研究では、多様な言語を網羅しかつ言語のバランスのとれたコーパスを用いて多言語言語モデルを学習し、様々なタスクにおけるfew-shot学習とzero-shot学習の性能を調査する。その結果、75億のパラメータを持つ最大規模のモデルが20以上の代表的な言語における少数ショット学習において新たなSoTAを示し、多言語推論や自然言語推論において同一サイズのGPT-3を上回る性能を示しました。機械翻訳ベンチマークFLORES-101では、182の翻訳方向のうち171方向でGPT-3を上回り、45方向で教師ありベースラインを上回った。</p>
<p>また、本モデルが成功・失敗した点を詳細に分析し、いくつかのタスクにおいて言語横断的な学習を可能にしていることを明らかにした。そして、5つの言語でのヘイトスピーチ検出などの社会的価値のあるタスクで言語モデルを評価し、GPT-3モデルと同様の限界があることもわかった。</p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">論文紹介</a></div><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-606baee39e6f2570" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/02/11/research_2022_02_11/"><span class="level-item">論文メモ：多言語音声言語モデルmSLAM、ニューラルネットワークの忘却は必要、事前学習モデルを効率化するpNLP-Mixer</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/self_face.png" alt="金子 正弘"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">金子 正弘</p><p class="is-size-6 is-block">東京工業大学 博士研究員</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>東京</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">投稿</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">カテゴリ</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">タグ</p><a href="/tags"><p class="title">11</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://www.getrevue.co/profile/masahirokaneko_" target="_blank" rel="noopener">ニュースレターを登録する</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Homepage" href="https://sites.google.com/view/masahirokaneko/日本語"><i class="fas fa-home"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/MasahiroKaneko_"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Google Scholar" href="https://scholar.google.co.jp/citations?user=c-CLPD0AAAAJ&amp;hl=ja"><i class="fas fa-graduation-cap"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/kanekomasahiro"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Gist" href="https://gist.github.com/kanekomasahiro"><i class="fas fa-code"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">リンク</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.nlp.c.titech.ac.jp" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">岡崎研究室</span></span><span class="level-right"><span class="level-item tag">www.nlp.c.titech.ac.jp</span></span></a></li><li><a class="level is-mobile" href="https://livnlp.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ダヌシカ研究室</span></span><span class="level-right"><span class="level-item tag">livnlp.github.io</span></span></a></li><li><a class="level is-mobile" href="http://cl.sd.tmu.ac.jp" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">小町研究室</span></span><span class="level-right"><span class="level-item tag">cl.sd.tmu.ac.jp</span></span></a></li></ul></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label"> </h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4881949383702908" data-ad-slot="4108061319" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">カテゴリ</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%F0%9F%92%BB-%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0/"><span class="level-start"><span class="level-item">💻 プログラミング</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/"><span class="level-start"><span class="level-item">🔎 研究‍</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/%F0%9F%9B%92-%E8%B2%B7%E3%81%84%E7%89%A9/"><span class="level-start"><span class="level-item">🛒 買い物</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最近の記事</h3><article class="media"><figure class="media-left"><a class="image" href="/2022/02/18/research_2022_02_18/"><img src="/images/survey.png" alt="論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-17T15:00:00.000Z">2022-02-18</time></p><p class="title"><a href="/2022/02/18/research_2022_02_18/">論文メモ：訓練不要な層に置換することによる高速化、翻訳モデルの出力による評価モデルの弱点分析、学習設定のデータスケーリング法則への影響、多言語言語モデルXGLM</a></p><p class="categories"><a href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/02/11/research_2022_02_11/"><img src="/images/survey.png" alt="論文メモ：多言語音声言語モデルmSLAM、ニューラルネットワークの忘却は必要、事前学習モデルを効率化するpNLP-Mixer"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-10T15:00:00.000Z">2022-02-11</time></p><p class="title"><a href="/2022/02/11/research_2022_02_11/">論文メモ：多言語音声言語モデルmSLAM、ニューラルネットワークの忘却は必要、事前学習モデルを効率化するpNLP-Mixer</a></p><p class="categories"><a href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/02/04/research_2022_02_04/"><img src="/images/survey.png" alt="論文メモ：CNNとTransformer事前学習モデルの比較、ルールベースと深層学習を統合した手法DEEPCTRL、アテンション機構の説明性に対する忠実性の調査"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-03T15:00:00.000Z">2022-02-04</time></p><p class="title"><a href="/2022/02/04/research_2022_02_04/">論文メモ：CNNとTransformer事前学習モデルの比較、ルールベースと深層学習を統合した手法DEEPCTRL、アテンション機構の説明性に対する忠実性の調査</a></p><p class="categories"><a href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/02/02/research_2022_02_01/"><img src="/images/survey.png" alt="論文メモ：知識蒸留と枝刈りによる公平性改善、モデル出力の一貫性を評価するDiscoScore、寄与率による説明性に対する人間の理解、特徴量による説明性は人間理解への貢献を検証"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-01T15:00:00.000Z">2022-02-02</time></p><p class="title"><a href="/2022/02/02/research_2022_02_01/">論文メモ：知識蒸留と枝刈りによる公平性改善、モデル出力の一貫性を評価するDiscoScore、寄与率による説明性に対する人間の理解、特徴量による説明性は人間理解への貢献を検証</a></p><p class="categories"><a href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2022/01/30/research_2022_01_30/"><img src="/images/survey.png" alt="論文メモ：ブラックボックス設定のプロンプト学習、プログラム形式の推論のための事前学習POET、経済的公平さと繁栄のための民主的AI"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-01-29T15:00:00.000Z">2022-01-30</time></p><p class="title"><a href="/2022/01/30/research_2022_01_30/">論文メモ：ブラックボックス設定のプロンプト学習、プログラム形式の推論のための事前学習POET、経済的公平さと繁栄のための民主的AI</a></p><p class="categories"><a href="/categories/%F0%9F%94%8E-%E7%A0%94%E7%A9%B6%E2%80%8D/">🔎 研究‍</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">アーカイブ</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">2月 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/06/"><span class="level-start"><span class="level-item">6月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">5月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">4月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">タグ</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88/"><span class="tag">データセット</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E3%83%9E%E3%82%B9%E3%82%AF%E4%BB%98%E3%81%8D%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/"><span class="tag">マスク付き言語モデル</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%AC%E5%B9%B3%E6%80%A7/"><span class="tag">公平性</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%98%E8%AA%9E%E5%88%86%E6%95%A3%E8%A1%A8%E7%8F%BE/"><span class="tag">単語分散表現</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B7%AE%E5%88%A5%E7%9A%84%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E9%99%A4%E5%8E%BB/"><span class="tag">差別的バイアス除去</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A/"><span class="tag">振り返り</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%B3%95%E8%AA%A4%E3%82%8A%E8%A8%82%E6%AD%A3/"><span class="tag">文法誤り訂正</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B5%81%E6%9A%A2%E6%80%A7/"><span class="tag">流暢性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/"><span class="tag">言語モデル</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A9%95%E4%BE%A1%E6%89%8B%E6%B3%95/"><span class="tag">評価手法</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/"><span class="tag">論文紹介</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/title.png" alt="金子の進捗" height="28"></a><p class="is-size-7"><span>&copy; 2022 金子 正弘</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ja");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="トップに戻る" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "このウェブサイトはあなたの経験を改善するためにCookieを使用しています。",
          dismiss: "了解しました",
          allow: "Cookiesを許可する",
          deny: "拒否する",
          link: "もっと詳しく知る",
          policy: "Cookieポリシー",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="何かを入力してください..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"何かを入力してください...","untitled":"(無題)","posts":"投稿","pages":"ページ","categories":"カテゴリ","tags":"タグ"});
        });</script></body></html>